{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Code based on https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
        "\n",
        "In this exercise, we are going to implement a [UNet-like](https://arxiv.org/pdf/1505.04597.pdf) architecture for the semantic segmentation task. \n",
        "The model is trained on the [CamVid](https://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/) dataset.\n",
        "\n",
        "Tasks:\n",
        "\n",
        "    1. Implement the missing pieces in the code.\n",
        "    2. Check that the given implementation reaches over 70% test accuracy after 10-15 epochs.\n",
        "    3. Check how segmentations produced by the model compare to ground truth segementations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfIXmJ-dRXfE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxGS_WsORXfF"
      },
      "outputs": [],
      "source": [
        "class UNetConvolutionStack(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel):\n",
        "        super(UNetConvolutionStack, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channel),\n",
        "            nn.LeakyReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyN2g-yQRXfG"
      },
      "outputs": [],
      "source": [
        "class EncoderStack(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, first_layer=False):\n",
        "        super(EncoderStack, self).__init__()\n",
        "        if first_layer:\n",
        "            self.down = nn.Sequential(\n",
        "                UNetConvolutionStack(in_channel, out_channel),\n",
        "                UNetConvolutionStack(out_channel, out_channel),\n",
        "            )\n",
        "        else:\n",
        "            self.down = nn.Sequential(\n",
        "                nn.MaxPool2d((2, 2)),\n",
        "                UNetConvolutionStack(in_channel, out_channel),\n",
        "                UNetConvolutionStack(out_channel, out_channel),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.down(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dp2-OwXORXfG"
      },
      "outputs": [],
      "source": [
        "class DecoderStack(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel):\n",
        "        super(DecoderStack, self).__init__()\n",
        "        self.upsample = nn.ConvTranspose2d(\n",
        "            in_channel, in_channel, 3, stride=2, padding=1\n",
        "        )\n",
        "        self.up = nn.Sequential(\n",
        "            UNetConvolutionStack(in_channel + out_channel, out_channel),\n",
        "            UNetConvolutionStack(out_channel, out_channel),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # TODO: implement skip connections.\n",
        "        # hint: x is the output of previous decoder layer,\n",
        "        # y is the output of corresponding encoder layer.\n",
        "        # Based on the arguments of the constructor,\n",
        "        # how should x and y be combined?\n",
        "        ...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBPeqMNSRXfG"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, encoder_channels, decoder_channels, num_classes):\n",
        "        super(UNet, self).__init__()\n",
        "        self.encoder = nn.ModuleList()\n",
        "        self.decoder = nn.ModuleList()\n",
        "        self.conv = nn.Conv2d(\n",
        "            decoder_channels[-1], num_classes, kernel_size=3, padding=1\n",
        "        )\n",
        "\n",
        "        encoder_sizes = zip(\n",
        "            range(len(encoder_channels)), encoder_channels, encoder_channels[1:]\n",
        "        )\n",
        "        for idx, in_size, out_size in encoder_sizes:\n",
        "            if idx > 0:\n",
        "                self.encoder.append(EncoderStack(in_size, out_size))\n",
        "            else:\n",
        "                self.encoder.append(EncoderStack(in_size, out_size, first_layer=True))\n",
        "\n",
        "        decoder_sizes = zip(decoder_channels, decoder_channels[1:])\n",
        "        for in_size, out_size in decoder_sizes:\n",
        "            self.decoder.append(DecoderStack(in_size, out_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: implement UNet's forward pass.\n",
        "        # hint: Remember to store outputs of subsequent\n",
        "        # encoder layers to use as input to decoder layers!\n",
        "        # Do not forget about the final convolution.\n",
        "        ...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AKH3oUqRXfH"
      },
      "outputs": [],
      "source": [
        "def train(model, device, train_loader, optimizer, epoch, log_interval):\n",
        "    model.train()\n",
        "    correct = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        log_probs = F.log_softmax(output, dim=1)\n",
        "        loss = F.nll_loss(log_probs, target)\n",
        "        pred = log_probs.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            _, _, image_width, image_height = data.size()\n",
        "            print(\n",
        "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
        "                    epoch,\n",
        "                    batch_idx * len(data),\n",
        "                    len(train_loader.dataset),\n",
        "                    100.0 * batch_idx / len(train_loader),\n",
        "                    loss.item(),\n",
        "                )\n",
        "            )\n",
        "    print(\n",
        "        \"Train accuracy: {}/{} ({:.0f}%)\".format(\n",
        "            correct,\n",
        "            (len(train_loader.dataset) * image_width * image_height),\n",
        "            100.0 * correct / (len(train_loader.dataset) * image_width * image_height),\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            log_probs = F.log_softmax(output, dim=1)\n",
        "            test_loss += F.nll_loss(\n",
        "                log_probs, target, reduction=\"sum\",\n",
        "            ).item()  # sum up batch loss\n",
        "            pred = log_probs.argmax(\n",
        "                dim=1, keepdim=True\n",
        "            )  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    _, _, image_width, image_height = data.size()\n",
        "    test_loss /= len(test_loader.dataset) * image_width * image_height\n",
        "\n",
        "    print(\n",
        "        \"Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
        "            test_loss,\n",
        "            correct,\n",
        "            (len(test_loader.dataset) * image_width * image_height),\n",
        "            100.0 * correct / (len(test_loader.dataset) * image_width * image_height),\n",
        "        )\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ed1Rwhv-RXfH"
      },
      "outputs": [],
      "source": [
        "batch_size = 50\n",
        "test_batch_size = 1000\n",
        "epochs = 15\n",
        "lr = 2e-3\n",
        "use_cuda = True\n",
        "seed = 1\n",
        "log_interval = 10\n",
        "test_size = 80\n",
        "data_root = '../data'\n",
        "\n",
        "imagenet_mean = [0.485, 0.456, 0.406]\n",
        "imagenet_std = [0.229, 0.224, 0.225]\n",
        "num_classes = 32\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ht3SPPVlRXfH"
      },
      "outputs": [],
      "source": [
        "use_cuda = use_cuda and torch.cuda.is_available()\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "train_kwargs = {\"batch_size\": batch_size}\n",
        "test_kwargs = {\"batch_size\": test_batch_size}\n",
        "if use_cuda:\n",
        "    cuda_kwargs = {\"num_workers\": 1, \"pin_memory\": True, \"shuffle\": True}\n",
        "    train_kwargs.update(cuda_kwargs)\n",
        "    test_kwargs.update(cuda_kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dD85qSzwRXfI"
      },
      "outputs": [],
      "source": [
        "class SegmentationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_dir, filenames, transform=None, target_transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.filenames = filenames\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.transformed_data_dir = os.path.join(root_dir, 'transformed')\n",
        "        if not os.path.exists(self.transformed_data_dir):\n",
        "            os.mkdir(self.transformed_data_dir)\n",
        "            os.mkdir(os.path.join(self.transformed_data_dir, 'images'))\n",
        "            os.mkdir(os.path.join(self.transformed_data_dir, 'labels'))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filename = self.filenames[idx]\n",
        "\n",
        "        transformed_image_path = os.path.join(self.transformed_data_dir, 'images', filename)\n",
        "        transformed_target_path = os.path.join(self.transformed_data_dir, 'labels', filename)\n",
        "        if os.path.exists(transformed_image_path) and os.path.exists(transformed_target_path):\n",
        "            img = torch.load(transformed_image_path)\n",
        "            target = torch.load(transformed_target_path)\n",
        "        else:\n",
        "            image_path = os.path.join(self.root_dir, '701_StillsRaw_full', f'{filename}.png')\n",
        "            target_path = os.path.join(self.root_dir, 'labels', f'{filename}_L.png')\n",
        "\n",
        "            img = Image.open(image_path).convert('RGB')\n",
        "            target = Image.open(target_path)\n",
        "\n",
        "            if self.transform is not None:\n",
        "                img = self.transform(img)\n",
        "            if self.target_transform is not None:\n",
        "                target = self.target_transform(target)\n",
        "            # Applying transformations takes a long. So we do this only once,\n",
        "            # then save transformed tensors to files and read them when they are needed again.\n",
        "            # This significantly speeds up next iterations over dataset.\n",
        "            torch.save(img, transformed_image_path)\n",
        "            torch.save(target, transformed_target_path)\n",
        "\n",
        "        return img, target"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dataset\n",
        "!mkdir -p {data_root}\n",
        "!wget -c -P {data_root} https://datasets.cms.waikato.ac.nz/ufdl/data/camvid/701_StillsRaw_full.zip\n",
        "!wget -c -P {data_root} https://datasets.cms.waikato.ac.nz/ufdl/data/camvid/LabeledApproved_full.zip\n",
        "!wget -c -P {data_root} https://datasets.cms.waikato.ac.nz/ufdl/data/camvid/label_colors.txt\n",
        "!if [[ ! -d {data_root}/701_StillsRaw_full ]]; then unzip {data_root}/701_StillsRaw_full.zip -d {data_root}; fi\n",
        "!if [[ ! -d {data_root}/labels ]]; then unzip {data_root}/LabeledApproved_full.zip -d {data_root}/labels; fi\n",
        "!rm {data_root}/701_StillsRaw_full/Seq05VD_f02610.png 2>/dev/null\n",
        "!rm {data_root}/labels/Seq05VD_f02610_L.png 2>/dev/null"
      ],
      "metadata": {
        "id": "iWNbkWQpKnOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7r9wpzeRXfI"
      },
      "outputs": [],
      "source": [
        "labels_colors = open(os.path.join(data_root, 'label_colors.txt'), 'r').readlines()\n",
        "labels_colors = [torch.tensor(list(map(int, line.split()[:3]))) for line in labels_colors]\n",
        "filenames = [fname[:-4] for fname in os.listdir(os.path.join(data_root, '701_StillsRaw_full'))]\n",
        "filenames_train = filenames[:-test_size]\n",
        "filenames_test = filenames[-test_size:]\n",
        "\n",
        "def rgb_to_single_color(x):\n",
        "    ret = torch.zeros(x.shape[1], x.shape[2], dtype=torch.long)\n",
        "    for single_color, rgb_colors in enumerate(labels_colors):\n",
        "        rgb_colors = rgb_colors.unsqueeze(1).unsqueeze(2)\n",
        "        indices = torch.all(torch.eq(x, rgb_colors), dim=0)\n",
        "        ret[indices] = single_color\n",
        "    return ret\n",
        "\n",
        "\n",
        "input_resize = transforms.Resize((240, 320))\n",
        "input_transform = transforms.Compose(\n",
        "    [\n",
        "        input_resize,\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(imagenet_mean, imagenet_std),\n",
        "    ]\n",
        ")\n",
        "\n",
        "target_resize = transforms.Resize((240, 320), interpolation=InterpolationMode.NEAREST)\n",
        "target_transform = transforms.Compose(\n",
        "    [\n",
        "        target_resize,\n",
        "        transforms.PILToTensor(),\n",
        "        transforms.Lambda(rgb_to_single_color),\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_dataset = SegmentationDataset(\n",
        "    data_root,\n",
        "    filenames_train,\n",
        "    transform=input_transform,\n",
        "    target_transform=target_transform,\n",
        ")\n",
        "test_dataset = SegmentationDataset(\n",
        "    data_root,\n",
        "    filenames_test,\n",
        "    transform=input_transform,\n",
        "    target_transform=target_transform,\n",
        ")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, **train_kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, **train_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1KtqXIPRXfJ",
        "outputId": "226945b4-4a1a-4217-82a8-ee4094dd1562"
      },
      "outputs": [],
      "source": [
        "model = UNet(\n",
        "    encoder_channels=[3, 8, 16, 32],\n",
        "    decoder_channels=[32, 16, 8],\n",
        "    num_classes=num_classes,\n",
        ").to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(model, device, train_loader, optimizer, epoch, log_interval)\n",
        "    test(model, device, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "data, target = next(iter(test_loader))\n",
        "output = model(data.to(device))\n",
        "pred = output.argmax(dim=1, keepdim=True).to('cpu')\n",
        "\n",
        "for i in range(5):\n",
        "    image = data[i].permute(1, 2, 0).numpy()\n",
        "    target_segmentation = target[i]\n",
        "    pred_segmentation = pred[i].squeeze()\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 20), gridspec_kw={'width_ratios': [1, 2]})\n",
        "    segmentations = torch.hstack([target_segmentation, pred_segmentation])\n",
        "    segmentations = segmentations.unsqueeze(2).numpy()\n",
        "    axes[0].imshow(np.clip(image, 0, 1))\n",
        "    axes[0].axis('off')\n",
        "    axes[1].imshow(segmentations)\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "9CHQGkXIKfbN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2Aa03GTRXfJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "6fa2fa4f4d9d3d9ca73eb3739cc0e85a72773041ed8c7376d5dc2c41e6946bf8"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
